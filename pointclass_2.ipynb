{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import provider\n",
    "import tensorflow as tf \n",
    "import tf_ops.grouping.tf_grouping as grouping\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers \n",
    "import time\n",
    "# import dataset\n",
    "import glob\n",
    "import pointnet_util as utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES=glob.glob(\"modelnet40_ply_hdf5_2048/*train*.h5\")\n",
    "TEST_FILES=glob.glob(\"modelnet40_ply_hdf5_2048/*test*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(keras.Model):\n",
    "    def __init__(self,out_dim,k_head):\n",
    "       super(self_attention,self).__init__()\n",
    "       self.out_dim=out_dim\n",
    "       self.k=k_head\n",
    "    def build(self,input_shape):\n",
    "        self.convq=[layers.Conv2D(self.out_dim,1,1) for i in range(self.k)]\n",
    "        self.convk=[layers.Conv2D(self.out_dim,1,1) for i in range(self.k)]\n",
    "        self.convv=[layers.Conv2D(self.out_dim,1,1) for i in range(self.k)]\n",
    "        self.Dense_out=layers.Dense(self.out_dim,activation=\"relu\")\n",
    "        self.norm=keras.layers.BatchNormalization()\n",
    "        self.soft=keras.layers.Softmax()\n",
    "    def call(self,input_feature,training=True):\n",
    "        out=[]\n",
    "        for i in range(self.k):\n",
    "            q=self.convq[i](input_feature)\n",
    "            v=self.convv[i](input_feature)\n",
    "            k=self.convk[i](input_feature)\n",
    "            q=tf.reduce_mean(q,axis=-2,keepdims=True)\n",
    "            # q=tf.tile(q,[1,1,tf.shape(v)[2],1])\n",
    "            k=tf.transpose(k,[0,1,3,2])\n",
    "            qk=tf.matmul(q,k)/tf.sqrt(tf.cast(self.out_dim,tf.float32))\n",
    "            out.append(tf.matmul(self.soft(qk),v))\n",
    "        out=tf.concat(out,axis=-1)\n",
    "        out=self.norm(self.Dense_out(tf.squeeze(out,axis=-2)),training=training)\n",
    "        return  out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(xyz):\n",
    "    num=tf.shape(xyz)[1]\n",
    "    return xyz[:,0:tf.cast(num/2,tf.int32),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pointcloud_class(keras.Model):\n",
    "    def __init__(self,class_num=10):\n",
    "        super(pointcloud_class,self).__init__()\n",
    "        self.class_num=class_num\n",
    "    def build(self,inputshape):\n",
    "        self.n=7\n",
    "        self.Dense1=keras.layers.Dense(64,activation=\"relu\") \n",
    "        self.Dense2=keras.layers.Dense(128,activation=\"relu\") \n",
    "        self.norm1=keras.layers.BatchNormalization()\n",
    "        self.norm2=keras.layers.BatchNormalization()\n",
    "        self.self_attention=[self_attention(128,1) for i in range(self.n+1)]\n",
    "        self.Dense=[layers.Dense(128,activation=\"relu\") for _ in range(self.n)]\n",
    "        self.Dense3=layers.Dense(self.class_num)\n",
    "        self.soft=keras.layers.Softmax()\n",
    "    def call(self , points_xyz,training=True):\n",
    "        # featrue=tf.concat([points_xyz,Density],axis=-1)\n",
    "        featrue=self.Dense1(points_xyz)\n",
    "        featrue=self.norm1(featrue,training=training)\n",
    "        featrue=self.Dense2(featrue)\n",
    "        featrue=self.norm2(featrue,training=training)\n",
    "        old_xyz=points_xyz\n",
    "        for i in range(self.n):\n",
    "            new_xyz = random_sample(old_xyz)\n",
    "            new_xyz, grouped_featur, idx, grouped_xyz=utils.\n",
    "            grouped_feature=grouping.group_point(featrue,idx)\n",
    "            new_xyz=tf.reduce_mean(grouped_xyz,axis=-2)\n",
    "            local_fature=grouped_feature-tf.reduce_mean(grouped_feature,axis=-2,keepdims=True)\n",
    "            global_fature=tf.reduce_max(grouped_featur,axis=-2,keepdims=False)\n",
    "            local_featrue=self.self_attention[i](local_fature,training=training)\n",
    "            featrue=self.Dense[i](tf.concat([local_featrue,global_fature],axis=-1))\n",
    "            old_xyz=new_xyz\n",
    "        grouped_xyz, new_points, idx=utils.grouping_all(featrue,new_xyz)\n",
    "        out=self.self_attention[self.n](new_points,training=training)\n",
    "        out=tf.squeeze(out,axis=1)\n",
    "        out=self.Dense3(out)\n",
    "        out=self.soft(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule=keras.optimizers.schedules.ExponentialDecay(0.001,100000,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer=keras.optimizers.Adam(0.0001)\n",
    "loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model=pointcloud_class(40)\n",
    "mtric1=keras.metrics.SparseCategoricalAccuracy()\n",
    "mtric2=keras.metrics.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FOUT = open('log_train.txt', 'w')\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shuffle(data):\n",
    "#     shuffed_data= np.zeros(data.shape, dtype=np.float32)\n",
    "#     for k in range(data.shape[0]):\n",
    "#         shuffed_data[k,...]=np.random.shuffle(data[k,...])\n",
    "#     return shuffed_data\n",
    "@tf.function\n",
    "def callmodel(input,current_label):\n",
    "    with tf.GradientTape() as tape:\n",
    "            logits=model(input,training=True)\n",
    "            if model.losses :\n",
    "                regularization_loss=tf.math.add_n(model.losses)\n",
    "            else:\n",
    "                regularization_loss=0\n",
    "            loss_value = loss(current_label, logits)+regularization_loss\n",
    "            # print(loss_value)\n",
    "    grads=tape.gradient(loss_value,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    mtric1.update_state(current_label, logits)\n",
    "    mtric2.update_state(current_label, logits)\n",
    "@tf.function\n",
    "def evaluate_model(input,current_label):\n",
    "    logits=model(input,training=False)\n",
    "    mtric1.update_state(current_label, logits)\n",
    "    mtric2.update_state(current_label, logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffledata(data):\n",
    "    idx=np.arange(data.shape[1])\n",
    "    for k in range(data.shape[0]):\n",
    "        np.random.shuffle(idx)\n",
    "        data[k,...]=data[k,idx,:]\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(epoch):\n",
    "    # train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    # np.random.shuffle(train_file_idxs)\n",
    "    NUM_POINT=1024\n",
    "    BATCH_SIZE=128\n",
    "    global BATCH\n",
    "    for step,(current_data,current_label) in enumerate(datasets):\n",
    "        # print(current_data.shape[0])\n",
    "        BATCH=BATCH+1\n",
    "        ckpt.batch.assign_add(1)\n",
    "        # Augment batched point clouds by rotation and jittering\n",
    "        rotated_data = provider.rotate_point_cloud(current_data.numpy())\n",
    "        jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "        jittered_data= jittered_data.astype(np.float32)\n",
    "        jittered_data=shuffledata(jittered_data)\n",
    "        # jittered_data=shuffle(jittered_data)\n",
    "        # print(jittered_data.dtype)\n",
    "        # print(jittered_data.shape)\n",
    "        tf.keras.backend.set_value(optimizer.lr, lr_schedule(BATCH*BATCH_SIZE))\n",
    "        callmodel(jittered_data,current_label)\n",
    "        # pred_val = np.argmax(logits, 1)\n",
    "        # correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        # total_correct += correct\n",
    "        # total_seen += BATCH_SIZE\n",
    "        # # print(loss_value)\n",
    "        # loss_sum += float(loss_value)\n",
    "def test_one_epoch(epoch):\n",
    "    for setp ,(data,label) in enumerate(test_datasets):\n",
    "        evaluate_model(data,label)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt=tf.train.Checkpoint(model=model,opti=optimizer,batch=tf.Variable(1))\n",
    "ckpt_mana=tf.train.CheckpointManager(ckpt,\"pointcloud_class\",max_to_keep=3)\n",
    "epochs=200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[];label=[]\n",
    "\n",
    "for i in range(len(TRAIN_FILES)):\n",
    "    data1,label1=provider.loadDataFile(TRAIN_FILES[i])\n",
    "    data.append(data1[:,0:1024,:])\n",
    "    label.append(label1)\n",
    "data=np.concatenate(data,axis=0)\n",
    "label=np.concatenate(label,axis=0)\n",
    "datasets=tf.data.Dataset.from_tensor_slices((data,label))\n",
    "datasets=datasets.shuffle(buffer_size=1024)\n",
    "func=lambda x,y:(tf.random.shuffle(x) ,y)\n",
    "datasets=datasets.map(func,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "datasets=datasets.batch(256)\n",
    "# datasets.cache()\n",
    "\n",
    "datasets=datasets.prefetch(1)\n",
    "\n",
    "data=[];label=[]\n",
    "\n",
    "for i in range(len(TEST_FILES)):\n",
    "    data1,label1=provider.loadDataFile(TEST_FILES[i])\n",
    "    data.append(data1[:,0:1024,:])\n",
    "    label.append(label1)\n",
    "data=np.concatenate(data,axis=0)\n",
    "label=np.concatenate(label,axis=0)\n",
    "test_datasets=tf.data.Dataset.from_tensor_slices((data,label))\n",
    "test_datasets=test_datasets.shuffle(buffer_size=1024)\n",
    "func=lambda x,y:(tf.random.shuffle(x) ,y)\n",
    "test_datasets=test_datasets.map(func,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_datasets=test_datasets.batch(256)\n",
    "# datasets.cache()\n",
    "test_datasets=test_datasets.prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nStart of epoch 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    train_one_epoch(epoch)\n",
    "    ckpt_mana.save()\n",
    "    # print(lr_schedule)\n",
    "    log_string(\"-----{}------\".format(epoch))\n",
    "    log_string('train_mean loss: %f' % float(mtric2.result())) \n",
    "    log_string('train_accuracy: %f' % float(mtric1.result()))\n",
    "    mtric1.reset_states()\n",
    "    mtric2.reset_states()\n",
    "    test_one_epoch(epoch)\n",
    "    log_string('test_mean loss: %f' % float(mtric2.result()))\n",
    "    log_string('test_accuracy: %f' % float(mtric1.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}